{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 – Markov Decision Processes\n",
    "\n",
    "The primary description of this coursework is available on course page page. This is the Jupyter notebook you must complete and submit to receive marks. \n",
    "\n",
    "**You must follow all instructions given in this notebook.**\n",
    "\n",
    "Restart the kernel and run all cells before submitting the notebook. This will guarantee that we will be able to run your code for testing.\n",
    "\n",
    "Remember to save your work regularly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Details\n",
    "This assignment has two parts, both are contained within this one notebook. You are welcome to write code separately (e.g. in an IDE like Pycharm) which you then copy into the notebook to submit, but you must make sure the notebook runs without any additional resources.\n",
    "\n",
    "In part one you will write code to solve a simple dice game using value iteration, the details of which are explained below. This part is worth 80% of the assignment grade.\n",
    "\n",
    "In part two you will write about your implementation, explain any choices you made, and discuss the possible impact of changing the game mechanics. This part is worth 20% of the assignment grade.\n",
    "\n",
    "## A Dice Game\n",
    "The goal of the assignment is to solve the optimal strategy for a simple single-player points-based dice game using value iteration. The rules of the basic version of the game are as follows:\n",
    "* You start with 0 points\n",
    "* Roll three fair six-sided dice\n",
    "* Now choose:\n",
    " * Stick, accept the values shown. If two or more dice show the same values, then all of them are flipped upside down: 1 becomes 6, 2 becomes 5, 3 becomes 4, and vice versa. The total is then added to your points and this is your final score.\n",
    " * OR reroll the dice. You may choose to hold any combination of the dice on the current value shown. Rerolling costs you 1 point – so during the game and perhaps even at the end your score may be negative. You then make this same choice again.\n",
    " \n",
    "The best possible score for this game is 18 and is achieved by rolling three 1s on the first roll.\n",
    "\n",
    "The reroll penalty prevents you from rolling forever to get this score. If the value of the current dice is greater than the expected value of rerolling them (accounting for the penalty), then you should stick.\n",
    "\n",
    "The optimal decision is independent of your current score. It does not matter whether it is your first roll with a current score of 0, or your twentieth roll with a current score of -19 (in which case a positive end score is impossible), in either of these cases if you roll three 6s (which, if you stick, will only add 3 points) then you still expect to get a *better* end score by rerolling and taking the penalty. Almost any other roll will beat it, so it's still the right choice to maximise your score.\n",
    "\n",
    "It is pretty obvious that you should stick on three 1s, and reroll on three 6s. Should you hold any of the 6s when you reroll? What about other values? What should you do if the dice come up 3, 4, 5?\n",
    "\n",
    "We do not know what numbers will come up when we roll, but we do know exactly what the probability of any given roll is. This is the point of the probabilistic reasoning section of the unit; if we can model the true probabilities then we can mathematically calculate the optimal policy. Not all real world situations use dice, but these techniques work well even if we can only estimate the true probabilities.\n",
    "\n",
    "You can play the game if you uncomment and run the line in the cell below. *Make sure to comment out this cell before submitting.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the line below to play the game.\n",
    "# You *must* comment it out again before submitting.\n",
    "# %run dice_game.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "The algorithm you should use to solve this game is called value iteration. It works by calculating the expected value for each action available from each state, and taking the maximum one. This value, for a given state and action, is calculated by taking a sum of the immediate reward and the expected value of the next state (which itself must be calculated as a probability-weighted sum of expected values for each possible next state). Since the values used in the second part of the sum are just estimates, the process must be repeated, updating each state, until convergence within a certain tolerance.\n",
    "\n",
    "Here is some pseudocode for the value iteration algorithm. <br />\n",
    "\n",
    "<img src=\"images/valueiteration.png\" width=400 />\n",
    "\n",
    "In the pseudocode above, $\\mathcal{S}$ refers to the set of all non-terminal states and $\\mathcal{S}^{+}$ refers to all states (including any terminal). Terminal states should always have a value of zero.\n",
    "\n",
    "This pseudocode is based on the one from Sutton and Barto (page 83). The same technique in Russel and Norvig (page 652) has a more complicated termination criteron, but assumes the discount factor $\\gamma < 1$. \n",
    "\n",
    "**For this assignment you should use $\\gamma = 1$.** Because of the conditions of the game it is only possible to get a non-negative score by holding all three dice and transitioning to a terminal state. Since we know the game will terminate, we can use non-discounted rewards. \n",
    "\n",
    "You may wish to consult either or both textbook descriptions to better your understanding of the technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dice Game Class\n",
    "A class called `DiceGame` is provided within `dice_game.py` for you to use in your solution. Here is a demonstration of its features.\n",
    "\n",
    "When you create a DiceGame object, by default you will get the rules as stated above. For part two of the assignment you may wish to modify the game mechanics, and you can do this using the constructor, for example:\n",
    "```python\n",
    "game = DiceGame(dice=4, sides=3, bias=np.array([0.1, 0.1, 0.8]), penalty=2)\n",
    "```\n",
    "will create a game where you roll 4 dice, each with 3 sides, where each one is far more likely to roll a 3 than they are to roll a 1 or a 2, and furthermore the penalty for rerolling is now 2 points instead of 1. *Note: this does not necessarily result in an interesting game.*\n",
    "\n",
    "Once created, the `DiceGame` object can be run in two different modes, *simulation* and *analysis*. For this assignment you will mostly use [*analysis* mode](#Analysis-Mode), but some understanding of simulation mode might be useful.\n",
    "\n",
    "### Simulation Mode\n",
    "The object provides the methods required to simulate playing the game. This might be useful if you want to test your policy, or you just want to try playing the game yourself, as we did in the cell earlier. The current dice values are found by calling `get_dice_state()`, they will always be listed in ascending order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dice_game import DiceGame\n",
    "import numpy as np\n",
    "\n",
    "# setting a seed for the random number generator makes testing easier!\n",
    "np.random.seed(111)\n",
    "\n",
    "game = DiceGame()\n",
    "game.get_dice_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To roll the dice, you call the `roll` method which takes one parameter: a tuple representing which dice you want to hold, numbered from zero. We rolled (2, 3, 4). Suppose we want to hold the 2, we would pass the tuple `(0,)` into the `roll` method (note we need to include the comma so that Python knows this is a tuple).\n",
    "\n",
    "The `roll` method returns a tuple containing: the reward for this action, the new state, and whether the game is over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward, new_state, game_over = game.roll((0,))\n",
    "print(reward)\n",
    "print(new_state)\n",
    "print(game_over)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose we are happy and wish to stick to get our final score. We can call the `roll` method and supply a tuple containing all three dice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward, new_state, game_over = game.roll((0, 1, 2))\n",
    "print(reward)\n",
    "print(new_state)\n",
    "print(game_over)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the return value is just the reward for the action, in this case 15. To get our final score we can inspect `game.score`. We rerolled once, so expect to get a score of 14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(game.score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to play again we can call `game.reset()` which returns the new starting dice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis Mode\n",
    "In analysis mode, you are not playing the game, but asking the object for all possible outcomes of certain actions. This will be used in the value iteration algorithm.\n",
    "\n",
    "First of all, it is useful to know that all the possible states and all the possible actions are stored inside the object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = DiceGame()\n",
    "print(f\"The first 5 of {len(game.states)} possible dice rolls are: {game.states[0:5]}\")\n",
    "print(f\"The possible actions on any given turn are: {game.actions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the most important method is `get_next_states(action, dice_state)`. This allows you to get all the possible resulting states for any given state and action.\n",
    "\n",
    "Earlier we had the roll of `(2, 3, 4)` and decided to hold the 2. The game can calculate all possible outcomes for us, and crucially will also give us the probability of each state occurring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, game_over, reward, probabilities = game.get_next_states((0,), (2, 3, 4))\n",
    "for state, probability in zip(states, probabilities):\n",
    "    print(f\"Would get roll of {state} with probability {probability}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method also works consistently when all dice are held, reporting that this action would cause the game to be over and giving the reward. Note that the list of states returned is empty, since we have no specific way to denote that a state is terminal. This is slightly inconsistent with simulation mode which shows the final dice after flipping duplicates (which is mainly for user friendliness)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, game_over, reward, probabilities = game.get_next_states((0, 1, 2), (2, 2, 5))\n",
    "print(game_over)\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part One\n",
    "Write your code for value iteration in the cell below. This part is worth 80% of the assignment.\n",
    "\n",
    "**Your solution must contain a method called `solve(game, theta)` which returns a dictionary mapping from each state to the best action.**\n",
    "* You must use the values of the two parameters `game` and `theta` in your solution:\n",
    " * `game` is a DiceGame object – you should use this to query the game as described [above](#Analysis-Mode). If you write your solution correctly, you should be able to change the game rules by changing the constructor (e.g. changing the number of dice), and your value iteration algorithm should still work.\n",
    " * `theta` is the tolerance value $\\theta$, after which you will assume convergence, as shown in the pseudocode [above](#Value-Iteration). You should experiment to determine what value of $\\theta$ you think is best.\n",
    "\n",
    "In the cell after next there are a few sample tests. You should run these tests to ensure your code is working correctly. ***Submissions which fail to meet these basic tests may receive zero marks.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3e4ab32caa85914919180ae2b6b40b94",
     "grade": false,
     "grade_id": "cell-41d9e9b8b731b6fc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dice_game import DiceGame\n",
    "\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class DiceGameAgent(ABC):\n",
    "    def __init__(self, game):\n",
    "        self.game = game\n",
    "    \n",
    "    @abstractmethod\n",
    "    def play(self, state):\n",
    "        pass\n",
    "\n",
    "class DiceAgent(DiceGameAgent):\n",
    "    def __init__(self, game):\n",
    "        \n",
    "        super().__init__(game)\n",
    "        self.num_actions = len(self.game.actions)\n",
    "        self.Values = self.solve()\n",
    "\n",
    "    \n",
    "    def expected_rewards(self, Values, state):\n",
    "        \n",
    "        Actions = {a:0 for a in self.game.actions}\n",
    "\n",
    "        # For each possible action\n",
    "        for a in Actions.keys():\n",
    "            \n",
    "            # Get the possible next_states, game_over, reward and probabilities of the next state occurring\n",
    "            next_states, game_over, reward, probabilities = self.game.get_next_states(a, state)\n",
    "            \n",
    "            # For each next_state and associated probability of that action & given state\n",
    "            for next_state, probability in zip(next_states, probabilities): \n",
    "                # If there is a next_state\n",
    "                if not game_over:\n",
    "                    # Calculate Bellman Update\n",
    "                    Actions[a] += probability * (reward+Values[next_state][0])\n",
    "                else:\n",
    "                    # next_state = None\n",
    "                    Actions[a] += probability * reward\n",
    "        return Actions\n",
    "\n",
    "\n",
    "    def solve(self, theta = 0.1):\n",
    "        \n",
    "        \n",
    "        # Initialise dictionary with each state as keys and values [expected reward, optimal policy]\n",
    "        Values = {i: [0, None] for i in self.game.states}\n",
    "        \n",
    "        # Loop until delta < self.theta\n",
    "        while True:\n",
    "            delta = 0\n",
    "    \n",
    "            # For each state\n",
    "            for s in self.game.states:\n",
    "                # Get the list of reward values pertaining to each action given that state\n",
    "                A = self.expected_rewards(Values, s)\n",
    "                \n",
    "                # Take the highest expected reward value \n",
    "                best_action_value = max(A.values())\n",
    "                \n",
    "                # Update delta with either delta or the absolute difference between the \n",
    "                # new maximum value for that state and the previous maximum value for that state\n",
    "                delta = max(delta, np.abs(best_action_value - Values[s][0]))\n",
    "                \n",
    "                # Update the maximum value and optimal policy for that state   \n",
    "                Values[s] = [best_action_value,max(A, key = A.get)]\n",
    "                \n",
    "            # If the change is less than self.theta, end the loop\n",
    "            if delta < theta:\n",
    "                break\n",
    "        return Values\n",
    "\n",
    "        \n",
    "    def play(self, state):\n",
    "        return self.Values[state][1]\n",
    "\n",
    "    \n",
    "game = DiceGame()\n",
    "agent = DiceAgent(game)\n",
    "policy  = agent.solve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All three tests pass.\n"
     ]
    }
   ],
   "source": [
    "# There should be exactly 56 states\n",
    "assert(len(policy.items()) == 56)\n",
    "\n",
    "# The optimal move when you roll three 1s is to stick\n",
    "assert(policy[(1, 1, 1)][1] == (0, 1, 2))\n",
    "\n",
    "# The optimal move when you roll three 4s is to reroll all 3\n",
    "assert(policy[(4, 4, 4)][1] == ())\n",
    "\n",
    "print(\"All three tests pass.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "12b9d1aeed6a5df4c944b924c218acb6",
     "grade": true,
     "grade_id": "cell-8cc6e99025c816a9",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'solve' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-cf294df7d8fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Get your policy, a dictionary mapping from state to (best) action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mpolicy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDiceGame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# There should be exactly 56 states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'solve' is not defined"
     ]
    }
   ],
   "source": [
    "# This cell contains three sample tests\n",
    "\n",
    "# Get your policy, a dictionary mapping from state to (best) action\n",
    "policy = solve(game=DiceGame())\n",
    "\n",
    "# There should be exactly 56 states\n",
    "assert(len(policy.keys()) == 56)\n",
    "\n",
    "# The optimal move when you roll three 1s is to stick\n",
    "assert(policy[(1, 1, 1)] == (0, 1, 2))\n",
    "\n",
    "# The optimal move when you roll three 4s is to reroll all 3\n",
    "assert(policy[(4, 4, 4)] == ())\n",
    "\n",
    "print(\"All three tests pass.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f37848216b8930a95767c0cd6544204b",
     "grade": false,
     "grade_id": "cell-68d0db82040c6acb",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "## Part Two\n",
    "For this part, you need to write a short discussion and analysis of your implementation in the cell below. This part is worth 20% of the assignment. \n",
    "* Short means approximately 300 words. There are no hard limits but overly long answers may be penalised.\n",
    "* You may wish to discuss topics such as:\n",
    " * Any choices you made during implementation\n",
    " * Any parameter values you used and the effect of choosing them differently\n",
    " * What impact there is on the results of your value iteration algorithm if run on a modified version of the game with different mechanics (as detailed [above](#Dice-Game-Class))\n",
    " * How efficient your code is and what improvements could be made"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cce925d5528efefba7d401c93752b8bb",
     "grade": true,
     "grade_id": "cell-faa4fb53acf82fb1",
     "locked": false,
     "points": 20,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "This agent chooses the most rewarded action to solve a game of dice.  The value iteration algorithm is applied to choose the best action, and the one-step look ahead heuristic to obtain the best score. A dictionary (“Values”) that contains every conceivable state as keys and a list of the expected utility/reward and the best policy for each state as values is computed during agent initialization. By iterating over each action accessible for each state, calculating every potential next state attainable from that action, as well as the related rewards gained and probability of reaching that state, the expected rewards are determined using the Bellman Update equation. As the one-step look ahead relies on next state values stored within “Values”, which are initialized to zero. The entire operation is carried out within a while-loop. The loop finally converges and ends when each state's predicted reward is updated or re-calibrated. Up till the change in predicted values is smaller than a set threshold, theta, the value iteration continues. The game starts whenever “Values” converges. By exploring the provided situation to find the best policy for each throw of the dice, the appropriate course of action is chosen. The agent will only ever carry out the stated action, hence it is important to note that the actions are deterministic in their nature. This makes it easier to calculate the Bellman update. I tested several values for the theta parameter, to determine the optimal value. Theta= 0.1 results in the quickest time per game and the highest score. PS I had to modify the cell containing the test code because otherwise i could not test my code. The agent satisfies the test that you provide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the end of the assignment. The cells below contain our test code for part one, *you must not delete or modify them.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d41026cb6d1c984e36e9c18935d1e697",
     "grade": true,
     "grade_id": "cell-8672ee0d94c8d964",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a TEST CELL. Do not delete or change. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fa03692e05978753cf2400014d6c9230",
     "grade": true,
     "grade_id": "cell-6d89af18ed2a5bab",
     "locked": true,
     "points": 30,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a TEST CELL. Do not delete or change. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6d94c2e037e39d7cf2339444d9563adb",
     "grade": true,
     "grade_id": "cell-3334c615802cb740",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a TEST CELL. Do not delete or change. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
